from mindspeed.features_manager.feature import MindSpeedFeature


class MLAFeature(MindSpeedFeature):

    def __init__(self):
        super().__init__('multi-latent-attention', optimization_level=2)

    def register_args(self, parser):
        group = parser.add_argument_group(title='multi latent attention')

        group.add_argument('--padded-base-length', type=int, default=128,
                            help='Fill Q K V of multi-latent-attention to an integer multiple of this parameter.')
        group.add_argument('--mla-fa-without-pad', action='store_true', default=False, 
                            help='Do not pad v_head_dim to q_head_dim in MLA')
        group.add_argument('--mla-mm-split', action='store_true', default=False, 
                            help='Split 2 up-proj matmul into 4 in MLA')
        group.add_argument("--mla-zero-memory", action='store_true', default=False, 
                            help="Save activation memory in multi-latent-attention.")
        group.add_argument("--mla-up-proj-tp-overlap", action='store_true', default=False, 
                            help='overlap up proj tp comm')
        group.add_argument("--recompute-mla-up-proj", action='store_true', default=False, 
                            help='recompute up projection in mla')
        group.add_argument('--mla-swap-core-attn-out', action='store_true', default=False, 
                            help='swap core_attn_out only in mla.')
        group.add_argument('--mla-fa-divide-qk', action='store_true', default=False,
                            help='Flash attn support mla with seperate q and k.')
        group.add_argument('--enable-mla-absorb', action='store_true', default=False,
                            help='Enable MLA up-projection matrix absorption.')
        group.add_argument('--enable-mla-scale-q-lora', action='store_true', default=False,
                            help='Enable MLA q lora scaling.')
        group.add_argument('--enable-mla-scale-kv-lora', action='store_true', default=False,
                            help='Enable MLA kv lora scaling.')    
        group.add_argument('--use-sparse-flash-attn', action='store_true', default=False,
                            help='Use sparse attention in multi-latent-attention.')
                            
    def validate_args(self, args):
        if args.multi_latent_attention:
            if args.kv_lora_rank is None:
                raise AssertionError('The parameter kv-lora-rank should be set when use multi_head_latent_attention.'
                )
            elif args.v_head_dim is None:
                raise AssertionError('The parameter v-head-dim should be set when use multi_head_latent_attention.'
                )
            elif args.qk_pos_emb_head_dim is None:
                raise AssertionError('The parameter qk-pos-emb-head-dim should be set when use multi_head_latent_attention.'
                )
            elif args.qk_head_dim is None:
                raise AssertionError('The parameter qk-head-dim should be set when use multi_head_latent_attention.'
                )

            if args.padded_base_length < 1:
                raise AssertionError('The value of padded_base_length cannot be less than 1.')
            if args.mla_up_proj_tp_overlap:
                if not args.mla_mm_split:
                    raise ValueError('--mla-up-proj-tp-overlap can only be used with mla-mm-split by now')
                if not args.sequence_parallel:
                    raise ValueError('--mla-up-proj-tp-overlap should be used with sequence parallel')
            if args.recompute_mla_up_proj:
                if not args.mla_up_proj_tp_overlap:
                    raise ValueError('--recompute-mla-up-proj can only be used with --mla-up-proj-tp-overlap')
                if args.mla_zero_memory:
                    raise ValueError('--recompute-mla-up-proj is incompatible with --mla-zero-memory')
            if args.mla_swap_core_attn_out:
                if args.schedules_method != "dualpipev":
                    raise AssertionError('--mla-swap-core-attn-out can only be used with dualpipev by now.')
                if not args.moe_fb_overlap:
                    raise AssertionError('--mla-swap-core-attn-out can only be used with --moe-fb-overlap by now.')
